# An√°lise Comparativa de Ferramentas de Teste de Desempenho com CI/CD

Este projeto realiza uma compara√ß√£o pr√°tica entre tr√™s das mais populares ferramentas de teste de desempenho de c√≥digo aberto: **k6**, **Locust** e **Apache JMeter**. O objetivo √© avaliar n√£o apenas a performance de cada ferramenta na gera√ß√£o de carga, mas tamb√©m a sua maturidade para automa√ß√£o em um pipeline de CI/CD moderno utilizando **GitHub Actions**.

A aplica√ß√£o alvo para os testes √© o **TeaStore**, uma aplica√ß√£o de e-commerce de refer√™ncia baseada em microsservi√ßos.

---

## üìä Mix de Endpoints e Cen√°rios de Teste

Os testes implementados simulam o comportamento real de usu√°rios navegando na loja online TeaStore. Cada cen√°rio segue um fluxo simplificado que representa as a√ß√µes mais comuns de um cliente.

### Tabela 1. Mix de Endpoints Testados

| Mix | % | Endpoints TeaStore | Observa√ß√µes |
|-----|---|-------------------|-------------|
| A - Login | 20% | /login | Autentica√ß√£o do usu√°rio |
| B - Home | 20% | /home, /products | P√°gina inicial e listagem de produtos |
| C - Categoria | 20% | /category?{id} | Navega√ß√£o por categoria (link extra√≠do dinamicamente) |
| D - Produto | 20% | /product?{id} | Visualiza√ß√£o de produto espec√≠fico (link extra√≠do dinamicamente) |
| E - Logout | 20% | /loginAction?logout | Encerramento da sess√£o |

**Fluxo do Teste:** Login ‚Üí Home ‚Üí Categoria ‚Üí Produto ‚Üí Logout

**Caracter√≠sticas T√©cnicas:**
- Extra√ß√£o din√¢mica de links para categorias e produtos (RegexExtractor no JMeter, BeautifulSoup no Locust, response.html() no k6)
- Cada usu√°rio virtual executa o fluxo completo uma vez por itera√ß√£o
- Medi√ß√£o de tempo de resposta, taxa de sucesso, throughput e detec√ß√£o de erros

### Tabela 2. Cen√°rios de Carga Implementados

| Cen√°rio | Usu√°rios Virtuais (VUs) | Dura√ß√£o | Ramp-up | Thresholds | Ferramentas |
|---------|-------------------------|---------|---------|------------|-------------|
| **Baixa Carga** | 100 | 2 minutos | 30s | p(95) < 500ms | k6, Locust, JMeter |
| **M√©dia Carga** | 500 | 3 minutos | 60s | p(95) < 1s | k6, Locust, JMeter |
| **Alta Carga** | 1000 | 5 minutos | 90s | p(95) < 2s | k6, Locust, JMeter |

**Total de Testes:** 9 cen√°rios (3 por ferramenta)
**Total de Relat√≥rios HTML:** 15 (5 JMeter + 5 Locust + 5 k6)

---

## üöÄ Ferramentas e Tecnologias Utilizadas

| Tecnologia | Finalidade |
| :--- | :--- |
| **k6 (Grafana)** | Ferramenta de teste de carga moderna, focada em "testes como c√≥digo" com scripts em JavaScript. |
| **Locust** | Ferramenta de teste de carga escrita em Python, conhecida por sua simplicidade e escalabilidade. |
| **Apache JMeter** | Ferramenta robusta e veterana para testes de carga e funcionais, baseada em Java. |
| **Docker** | Plataforma de cont√™ineres para executar a aplica√ß√£o e as ferramentas de teste em ambientes isolados. |
| **GitHub Actions** | Plataforma de CI/CD para automatizar a execu√ß√£o dos testes de desempenho a cada altera√ß√£o no c√≥digo. |

---

## ‚öôÔ∏è O Processo de Configura√ß√£o e Descoberta

A jornada para configurar este ambiente de testes automatizados foi um exerc√≠cio pr√°tico de engenharia e depura√ß√£o, seguindo os seguintes passos:

1.  **Configura√ß√£o do Ambiente Local:**
    * Instala√ß√£o e configura√ß√£o do **Docker Desktop** como base para a execu√ß√£o de cont√™ineres.
    * **Desafio:** A implanta√ß√£o da aplica√ß√£o **TeaStore** provou ser o maior obst√°culo inicial. M√∫ltiplas tentativas baseadas em documenta√ß√µes desatualizadas falharam, incluindo:
        * Busca por arquivos `docker-compose.yml` que n√£o existiam no c√≥digo-fonte.
        * Tentativa de compilar o projeto Java do zero, o que exigiu a instala√ß√£o de **JDK 17** e **Maven**.
    * **Solu√ß√£o:** A abordagem correta foi encontrada na documenta√ß√£o oficial do projeto (`GET_STARTED.md`), que indicava o uso de um arquivo `docker-compose.yaml` espec√≠fico, localizado em `examples/docker/`.

2.  **Desenvolvimento dos Scripts de Teste:**
    * Cria√ß√£o de um cen√°rio de **teste de carga simples** para as tr√™s ferramentas, simulando 10 usu√°rios simult√¢neos acessando a p√°gina inicial do TeaStore por 30 segundos.
    * Os scripts foram desenvolvidos para serem executados via Docker, utilizando `host.docker.internal` para comunica√ß√£o entre os cont√™ineres de teste e o cont√™iner da aplica√ß√£o.

3.  **Constru√ß√£o do Pipeline de CI/CD:**
    * Um workflow para **GitHub Actions** foi criado em `/.github/workflows/performance-tests.yml`.
    * O objetivo do pipeline √© automatizar todo o processo: baixar o c√≥digo, iniciar a aplica√ß√£o TeaStore e executar os testes de k6, Locust e JMeter em sequ√™ncia.

4.  **Depura√ß√£o e Ajuste Fino do Pipeline:**
    * **Desafio 1: Gatilho da Branch:** O pipeline n√£o era acionado porque o nome da branch local (`master`) n√£o correspondia ao configurado no arquivo (`main`).
    * **Desafio 2: Comando `docker-compose`:** A vers√£o do comando na m√°quina do GitHub Actions √© `docker compose` (sem h√≠fen), exigindo um ajuste no script.
    * **Desafio 3: Caminho do Arquivo:** O pipeline falhou ao n√£o encontrar o arquivo de configura√ß√£o do TeaStore, necessitando de uma corre√ß√£o precisa do caminho.
    * **Desafio 4: Artefatos do JMeter:** O JMeter se recusou a sobrescrever arquivos de relat√≥rio (`report/` e `results.jtl`), sendo necess√°rio adicionar passos de limpeza (`rm -rf` e `rm -f`) antes da execu√ß√£o do teste.

---

## üèÅ Estado Atual

O projeto agora possui um pipeline de CI/CD totalmente funcional que, a cada `push` para a branch `master`, executa automaticamente os testes de desempenho com as tr√™s ferramentas e salva o relat√≥rio detalhado do JMeter como um artefato, que pode ser baixado para an√°lise.

Este `README` serve como um registro do processo desafiador, mas bem-sucedido, de configura√ß√£o de um ambiente de testes de performance automatizados.

---

## üìä Resultados dos Testes de Desempenho

### Tabela 3. Lat√™ncia e Performance por Ferramenta

| Ferramenta | Cen√°rio | M√©dia (ms) | P95 (ms) | P99 (ms)* | Throughput (req/s) | Taxa Erro | Itera√ß√µes |
|------------|---------|------------|----------|-----------|-------------------|-----------|-----------|
| **K6** | 100 VUs | 419 | 1,270 | ~1,500 | 191.96 | 0.00% | 5,819 |
| **K6** | 500 VUs | 2,530 | 5,030 | ~5,800 | 186.90 | 0.00% | 5,862 |
| **K6** | 1000 VUs | 4,890 | 8,080 | ~9,500 | 194.99 | 0.00% | 6,303 |
| **JMeter** | 100 VUs | - | - | - | - | - | Pendente |
| **JMeter** | 500 VUs | - | - | - | - | - | Pendente |
| **JMeter** | 1000 VUs | - | - | - | - | - | Pendente |
| **Locust** | 100 VUs | - | - | - | - | - | Pendente |
| **Locust** | 500 VUs | - | - | - | - | - | Pendente |
| **Locust** | 1000 VUs | - | - | - | - | - | Pendente |

*P99 estimado com base na distribui√ß√£o P90-P95

### An√°lise K6 - Resultados Obtidos

**‚úÖ Taxa de Sucesso:**
- **100% de requisi√ß√µes bem-sucedidas** em todos os cen√°rios
- 0 falhas em 179,840 requisi√ß√µes totais (58,190 + 58,620 + 63,030)
- Todas as valida√ß√µes (checks) passaram: login, home, categoria, produto, logout

**üìà Escalabilidade:**
- **Lat√™ncia m√©dia:** 419ms (100 VUs) ‚Üí 2,530ms (500 VUs) ‚Üí 4,890ms (1000 VUs)
- **P95:** 1.27s (100 VUs) ‚Üí 5.03s (500 VUs) ‚Üí 8.08s (1000 VUs)
- Crescimento proporcional √† carga aplicada

**‚ö° Throughput:**
- **Est√°vel entre 187-195 req/s** em todos os cen√°rios
- Sistema mant√©m throughput consistente mesmo sob alta carga
- Limite de ~195 req/s sugere gargalo no SUT, n√£o no gerador

**üîÑ Dura√ß√£o dos Testes:**
- 100 VUs: 5m03s (303s)
- 500 VUs: 5m14s (314s)
- 1000 VUs: 5m23s (323s)

**üíæ Tr√°fego de Rede:**
- Data recebida: 133-144 MB (~430-446 kB/s)
- Data enviada: 9.6-10 MB (~31-32 kB/s)

**Status:** ‚úÖ K6 completo | ‚è≥ JMeter pendente | ‚è≥ Locust pendente
